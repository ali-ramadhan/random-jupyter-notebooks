{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDAnative, CuArrays\n",
    "using GPUifyLoops\n",
    "using OffsetArrays\n",
    "using BenchmarkTools\n",
    "using Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Adapt\n",
    "Adapt.adapt_structure(to, x::OffsetArray) = OffsetArray(Adapt.adapt(to, parent(x)), x.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increment integer with periodic wrapping.\n",
    "@inline incmod1(a, n) = ifelse(a==n, 1, a+1)\n",
    "\n",
    "# x, y, and z difference operators with periodic boundary conditions.\n",
    "# Nx, Ny, and Nz are the number of grid points in each dimension.\n",
    "# They return the difference at grid point (i, j, k).\n",
    "@inline δx(f, Nx, i, j, k) = @inbounds f[incmod1(i, Nx), j, k] - f[i, j, k]\n",
    "@inline δy(f, Ny, i, j, k) = @inbounds f[i, incmod1(j, Ny), k] - f[i, j, k]\n",
    "@inline δz(f, Nz, i, j, k) = @inbounds f[i, j, incmod1(k, Nz)] - f[i, j, k]\n",
    "\n",
    "# 3D Divergence operator.\n",
    "@inline div(f, Nx, Ny, Nz, Δx, Δy, Δz, i, j, k) = δx(f, Nx, i, j, k) / Δx + δy(f, Ny, i, j, k) / Δy + δz(f, Nz, i, j, k) / Δz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function div_kernel!(f, div_f)\n",
    "    Nx, Ny, Nz = size(f)\n",
    "    Δx, Δy, Δz = 1, 1, 1\n",
    "\n",
    "    # Calculate the divergence of f at every point and store it in div_f.\n",
    "    @loop for k in (1:Nz; blockIdx().z)\n",
    "        @loop for j in (1:Ny; (blockIdx().y - 1) * blockDim().y + threadIdx().y)\n",
    "            @loop for i in (1:Nx; (blockIdx().x - 1) * blockDim().x + threadIdx().x)\n",
    "                @inbounds div_f[i, j, k] = div(f, Nx, Ny, Nz, Δx, Δy, Δz, i, j, k)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @synchronize\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU test\n",
    "Nx, Ny, Nz = 32, 32, 16\n",
    "xc, yc = ones(Nx, Ny, Nz), rand(Nx, Ny, Nz);\n",
    "\n",
    "@launch CPU() div_kernel!(xc, yc)\n",
    "\n",
    "@test all(yc .== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark @launch CPU() div_kernel!($xc, $yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU test\n",
    "Nx, Ny, Nz = 32, 32, 16\n",
    "xg, yg = CuArray(ones(Nx, Ny, Nz)), CuArray(rand(Nx, Ny, Nz));\n",
    "\n",
    "Tx, Ty = 16, 16  # Threads per block\n",
    "Bx, By, Bz = Int(Nx/Tx), Int(Ny/Ty), Nz  # Blocks in grid.\n",
    "\n",
    "@launch CUDA() div_kernel!(xg, yg, threads=(Tx, Ty), blocks=(Bx, By, Bz))\n",
    "\n",
    "@test all(Array(yg) .== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "div (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@inline δx(f, Nx, i, j, k) = @inbounds f[i+1, j, k] - f[i, j, k]\n",
    "@inline δy(f, Ny, i, j, k) = @inbounds f[i, j+1, k] - f[i, j, k]\n",
    "\n",
    "@inline function δz(f, Nz, i, j, k)\n",
    "    if k == Nz\n",
    "        return 0\n",
    "    else\n",
    "        @inbounds return f[i, j, k+1] - f[i, j, k]\n",
    "    end\n",
    "end\n",
    "\n",
    "# 3D Divergence operator.\n",
    "@inline div(f, Nx, Ny, Nz, Δx, Δy, Δz, i, j, k) = δx(f, Nx, i, j, k) / Δx + δy(f, Ny, i, j, k) / Δy + δz(f, Nz, i, j, k) / Δz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "div_kernel_halo! (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function div_kernel_halo!(Nx, Ny, Nz, f, div_f)\n",
    "    Δx, Δy, Δz = 1, 1, 1\n",
    "\n",
    "    # Calculate the divergence of f at every point and store it in div_f.\n",
    "    @loop for k in (1:Nz; blockIdx().z)\n",
    "        @loop for j in (1:Ny; (blockIdx().y - 1) * blockDim().y + threadIdx().y)\n",
    "            @loop for i in (1:Nx; (blockIdx().x - 1) * blockDim().x + threadIdx().x)\n",
    "                @inbounds div_f[i, j, k] = div(f, Nx, Ny, Nz, Δx, Δy, Δz, i, j, k)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @synchronize\n",
    "    nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nx, Ny, Nz = 32, 32, 16\n",
    "\n",
    "hs = 1  # halo size\n",
    "xco = OffsetArray{Float64}(undef, 1-hs:Nx+hs, 1-hs:Ny+hs, 1:Nz);\n",
    "yco = OffsetArray{Float64}(undef, 1-hs:Nx+hs, 1-hs:Ny+hs, 1:Nz);\n",
    "xco.parent .= ones(size(xco)); yco.parent .= rand(size(yco)...);\n",
    "\n",
    "@launch CPU() div_kernel_halo!(Nx, Ny, Nz, xco, yco)\n",
    "\n",
    "@test all(yco[1:Nx, 1:Ny, 1:Nz] .== 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark @launch CPU() div_kernel_halo!(Nx, Ny, Nz, $xco, $yco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDAnative.KernelError",
     "evalue": "GPU compilation of #16(Int64, Int64, Int64, OffsetArray{Float32,3,CuArray{Float32,3}}, OffsetArray{Float32,3,CuArray{Float32,3}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 5 to your kernel function is of type OffsetArray{Float32,3,CuArray{Float32,3}}.\nThat type is not isbits, and such arguments are only allowed when they are unused by the kernel.\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of #16(Int64, Int64, Int64, OffsetArray{Float32,3,CuArray{Float32,3}}, OffsetArray{Float32,3,CuArray{Float32,3}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 5 to your kernel function is of type OffsetArray{Float32,3,CuArray{Float32,3}}.\nThat type is not isbits, and such arguments are only allowed when they are unused by the kernel.\n",
      "",
      "Stacktrace:",
      " [1] check_invocation(::CUDAnative.CompilerContext, ::LLVM.Function) at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/compiler/validation.jl:35",
      " [2] compile(::CUDAnative.CompilerContext) at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/compiler/driver.jl:94",
      " [3] #compile#109(::Bool, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::VersionNumber, ::Any, ::Any) at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/compiler/driver.jl:45",
      " [4] compile at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/compiler/driver.jl:43 [inlined]",
      " [5] #compile#108(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::CUDAdrv.CuDevice, ::Function, ::Any) at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/compiler/driver.jl:18",
      " [6] macro expansion at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/compiler/driver.jl:16 [inlined]",
      " [7] #cufunction#123(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(cufunction), ::getfield(GPUifyLoops, Symbol(\"##16#17\")){typeof(div_kernel_halo!)}, ::Type{Tuple{Int64,Int64,Int64,OffsetArray{Float32,3,CuArray{Float32,3}},OffsetArray{Float32,3,CuArray{Float32,3}}}}) at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/execution.jl:240",
      " [8] cufunction(::Function, ::Type) at /home/gridsan/aramadhan/.julia/packages/CUDAnative/PFgO3/src/execution.jl:240",
      " [9] macro expansion at /home/gridsan/aramadhan/.julia/packages/GPUifyLoops/xOg1y/src/GPUifyLoops.jl:109 [inlined]",
      " [10] macro expansion at ./gcutils.jl:87 [inlined]",
      " [11] #launch#34(::Base.Iterators.Pairs{Symbol,Tuple{Int64,Int64,Vararg{Int64,N} where N},Tuple{Symbol,Symbol},NamedTuple{(:threads, :blocks),Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64,Int64}}}}, ::Function, ::CUDA, ::typeof(div_kernel_halo!), ::Int64, ::Vararg{Any,N} where N) at /home/gridsan/aramadhan/.julia/packages/GPUifyLoops/xOg1y/src/GPUifyLoops.jl:106",
      " [12] (::getfield(GPUifyLoops, Symbol(\"#kw##launch\")))(::NamedTuple{(:threads, :blocks),Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64,Int64}}}, ::typeof(GPUifyLoops.launch), ::CUDA, ::typeof(div_kernel_halo!), ::Int64, ::Vararg{Any,N} where N) at ./none:0",
      " [13] top-level scope at /home/gridsan/aramadhan/.julia/packages/GPUifyLoops/xOg1y/src/GPUifyLoops.jl:50",
      " [14] top-level scope at In[6]:13"
     ]
    }
   ],
   "source": [
    "Nx, Ny, Nz = 32, 32, 16\n",
    "\n",
    "hs = 1\n",
    "\n",
    "x_underlying = CuArray{Float32}(ones(Nx+2*hs, Ny+2*hs, Nz))\n",
    "xgo = OffsetArray(x_underlying, 1-hs:Nx+hs, 1-hs:Ny+hs, 1:Nz);\n",
    "\n",
    "y_underlying = CuArray{Float32}(rand(Nx+2*hs, Ny+2*hs, Nz))\n",
    "ygo = OffsetArray(y_underlying, 1-hs:Nx+hs, 1-hs:Ny+hs, 1:Nz);\n",
    "\n",
    "Tx, Ty = 16, 16  # Threads per block\n",
    "Bx, By, Bz = Int(Nx/Tx), Int(Ny/Ty), Nz  # Blocks in grid.\n",
    "\n",
    "@launch CUDA() div_kernel_halo!(Nx, Ny, Nz, xgo, ygo, threads=(Tx, Ty), blocks=(Bx, By, Bz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: timings not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: timings not defined",
      "",
      "Stacktrace:",
      " [1] getproperty(::Module, ::Symbol) at ./sysimg.jl:13",
      " [2] top-level scope at In[7]:1"
     ]
    }
   ],
   "source": [
    "CUDAnative.timings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
